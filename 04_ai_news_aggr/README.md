# AI News Aggregator

![Python](https://img.shields.io/badge/python-3.12+-blue.svg)
![Docker](https://img.shields.io/badge/docker-enabled-blue.svg)
![PostgreSQL](https://img.shields.io/badge/postgresql-17-316192.svg)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991.svg)

An automated pipeline that scrapes AI news from YouTube, OpenAI, and Anthropic, generates intelligent summaries using LLMs, ranks them based on a personalized user profile, and sends a beautifully formatted daily email digest.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA COLLECTION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  YouTube Scraper  â”‚  OpenAI Scraper  â”‚  Anthropic Scraper       â”‚
â”‚  (RSS + API)      â”‚  (RSS Feed)      â”‚  (RSS + Docling)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                   â”‚                   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PostgreSQL DB      â”‚
                    â”‚  (4 Tables)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                   â”‚                   â”‚
           â†“                   â†“                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Digest     â”‚   â”‚  Curator     â”‚   â”‚  Email       â”‚
    â”‚  Agent      â”‚â†’  â”‚  Agent       â”‚â†’  â”‚  Agent       â”‚
    â”‚ (GPT-4o-mini)â”‚   â”‚ (GPT-4.1)    â”‚   â”‚(GPT-4o-mini) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â†“
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  HTML Email    â”‚
                                        â”‚  Newsletter    â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flow:**
1. **Scrapers** â†’ Fetch content from multiple sources
2. **PostgreSQL** â†’ Store raw data and processing state
3. **Digest Agent** â†’ Generate AI summaries (GPT-4o-mini)
4. **Curator Agent** â†’ Rank by user interests (GPT-4.1)
5. **Email Agent** â†’ Format and send personalized newsletter

## âœ¨ Features

- **ğŸ” Multi-Source Scraping**: Automatically fetches content from:
  - YouTube video transcripts
  - OpenAI official blog (RSS)
  - Anthropic blog (News, Research, Engineering via RSS)

- **ğŸ¤– Intelligent Summarization**: Uses GPT-4o-mini to generate concise, technical summaries of articles and videos

- **ğŸ¯ Personalized Curation**: Ranks news based on your specific interests, expertise level, and preferences defined in `user_profile.py`

- **ğŸ“§ Daily Email Digest**: Sends beautifully formatted HTML newsletters with top-ranked articles

- **ğŸ› ï¸ Robust Engineering**:
  - Dockerized PostgreSQL database
  - Pydantic validation for data integrity
  - Comprehensive error handling and retry logic
  - Structured logging throughout the pipeline

- **ğŸš€ Vibe Coding Ready**: Includes a complete `implementation_plan/` folder with exact prompts to recreate the entire project with AI assistance

## ğŸ”§ Tech Stack

- **Language**: Python 3.12+
- **AI**: OpenAI API (GPT-4.1, GPT-4o-mini)
- **Database**: PostgreSQL 17 (via Docker)
- **Key Libraries**:
  - `SQLAlchemy` - ORM and database management
  - `Pydantic` - Data validation
  - `feedparser` - RSS feed parsing
  - `youtube-transcript-api` - YouTube transcript extraction
  - `docling` - Web page to Markdown conversion
  - `markdown` - Markdown to HTML conversion

## ğŸ“¦ Installation & Setup

### Prerequisites

- Docker and Docker Compose
- Python 3.12+
- UV (recommended) or pip

### Step 1: Clone & Install Dependencies

```bash
# Clone the repository
git clone <repository-url>
cd ai_news_aggr

# Install dependencies with UV (recommended)
uv sync

# Or with pip
pip install -e .
```

### Step 2: Environment Configuration

```bash
# Copy the example environment file
cp app/example.env app/.env

# Edit app/.env and configure your secrets:
# - OPENAI_API_KEY (required)
# - ANTHROPIC_API_KEY (optional)
# - MY_EMAIL (for receiving digests)
# - APP_PASSWORD (Gmail app password)
# - Proxy credentials (optional)
```

### Step 3: Initialize Python Packages

```bash
# Ensure all directories have __init__.py files
python init_packages.py
```

### Step 4: Start Database

```bash
# Start PostgreSQL container
docker compose -f docker/docker-compose.yml up -d

# Create database tables
python app/database/create_tables.py
```

## ğŸš€ Usage

### Run the Complete Pipeline

```bash
# Run with defaults (last 24 hours, top 10 articles)
python main.py

# Look back 48 hours, send top 15 articles
python main.py 48 15

# Custom: last 72 hours, top 20 articles
python main.py 72 20
```

### Run Individual Services

```bash
# Run scrapers only
python -m app.runner

# Process YouTube transcripts
python -m app.services.process_youtube

# Process Anthropic markdown
python -m app.services.process_anthropic

# Generate digests
python -m app.services.process_digest

# Test email service
python -m app.services.email
```

### Run Tests

```bash
pytest
```

## ğŸ“ Project Structure

```
ai_news_aggr/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/              # AI agents (Digest, Curator, Email)
â”‚   â”‚   â”œâ”€â”€ curator_agent.py
â”‚   â”‚   â”œâ”€â”€ digest_agent.py
â”‚   â”‚   â””â”€â”€ email_agent.py
â”‚   â”œâ”€â”€ database/            # Database layer
â”‚   â”‚   â”œâ”€â”€ connection.py
â”‚   â”‚   â”œâ”€â”€ create_tables.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ repository.py
â”‚   â”œâ”€â”€ profiles/            # User profile configuration
â”‚   â”‚   â””â”€â”€ user_profile.py
â”‚   â”œâ”€â”€ scrapers/            # Data collection
â”‚   â”‚   â”œâ”€â”€ anthropic.py
â”‚   â”‚   â”œâ”€â”€ openai.py
â”‚   â”‚   â””â”€â”€ youtube.py
â”‚   â”œâ”€â”€ services/            # Processing services
â”‚   â”‚   â”œâ”€â”€ email.py
â”‚   â”‚   â”œâ”€â”€ process_anthropic.py
â”‚   â”‚   â”œâ”€â”€ process_curator.py
â”‚   â”‚   â”œâ”€â”€ process_digest.py
â”‚   â”‚   â”œâ”€â”€ process_email.py
â”‚   â”‚   â””â”€â”€ process_youtube.py
â”‚   â”œâ”€â”€ config.py            # Application configuration
â”‚   â”œâ”€â”€ daily_runner.py      # Daily pipeline orchestrator
â”‚   â”œâ”€â”€ example.env          # Environment template
â”‚   â””â”€â”€ runner.py            # Scraper registry
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml   # Database container definition
â”œâ”€â”€ implementation_plan/     # AI recreation prompts
â”‚   â”œâ”€â”€ 00_ROADMAP.md
â”‚   â”œâ”€â”€ 01_infra.md
â”‚   â”œâ”€â”€ 02_database.md
â”‚   â”œâ”€â”€ 03_scrapers.md
â”‚   â”œâ”€â”€ 04_agents.md
â”‚   â”œâ”€â”€ 05_services.md
â”‚   â”œâ”€â”€ 06_runners.md
â”‚   â”œâ”€â”€ 07_finalization.md
â”‚   â””â”€â”€ 08_tests.md
â”œâ”€â”€ init_packages.py         # Package initialization utility
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ pyproject.toml           # Project metadata & dependencies
â””â”€â”€ README.md                # This file
```

## ğŸ—ï¸ Developer Guide: Recreating with AI

This project includes a complete `implementation_plan/` folder containing the **exact prompts** used to build the entire application. You can recreate or extend this project using an AI coding assistant by following the prompts in order.

### How It Works

Each markdown file in `implementation_plan/` contains numbered prompts (e.g., `Prompt_01`, `Prompt_02`) with:
- **Task description**: What to build
- **Technical requirements**: Dependencies, APIs, data models
- **Code specifications**: Exact implementation details

### Execution Order

Follow this phase-by-phase execution order:

#### **Phase 1: Infrastructure Setup** (`01_infra.md`)
- **Prompt_01** â†’ Generate `pyproject.toml`
- **Prompt_02** â†’ Generate `docker/docker-compose.yml`
- **Prompt_03** â†’ Generate `app/example.env`
- **Manual Action**: Run `docker compose -f docker/docker-compose.yml up -d`

#### **Phase 2: Database Layer** (`02_database.md`)
- **Prompt_04** â†’ Generate `app/database/connection.py`
- **Prompt_05** â†’ Generate `app/database/models.py`
- **Prompt_06** â†’ Generate `app/database/repository.py`
- **Prompt_07** â†’ Generate `app/database/create_tables.py`
- **Manual Action**: Run `python app/database/create_tables.py`

#### **Phase 3: Scrapers** (`03_scrapers.md`)
- **Prompt_08** â†’ Generate `app/config.py`
- **Prompt_09** â†’ Generate `app/scrapers/youtube.py`
- **Prompt_10** â†’ Generate `app/scrapers/openai.py`
- **Prompt_11** â†’ Generate `app/scrapers/anthropic.py`

#### **Phase 4: AI Agents** (`04_agents.md`)
- **Prompt_12** â†’ Generate `app/profiles/user_profile.py`
- **Prompt_13** â†’ Generate `app/agents/digest_agent.py`
- **Prompt_14** â†’ Generate `app/agents/curator_agent.py`
- **Prompt_15** â†’ Generate `app/agents/email_agent.py`

#### **Phase 5: Services** (`05_services.md`)
- **Prompt_16** â†’ Generate `app/services/email.py`
- **Prompt_17** â†’ Generate `app/services/process_youtube.py`
- **Prompt_18** â†’ Generate `app/services/process_anthropic.py`
- **Prompt_19** â†’ Generate `app/services/process_digest.py`
- **Prompt_20** â†’ Generate `app/services/process_curator.py`
- **Prompt_21** â†’ Generate `app/services/process_email.py`

#### **Phase 6: Runners** (`06_runners.md`)
- **Prompt_22** â†’ Generate `app/runner.py`
- **Prompt_23** â†’ Generate `app/daily_runner.py`
- **Prompt_24** â†’ Generate `main.py`

#### **Phase 7: Finalization** (`07_finalization.md`)
- **Prompt_25** â†’ Generate `init_packages.py`
- **Manual Action**: Run `python init_packages.py`
- **Prompt_26** â†’ Generate `README.md` (this file)

#### **Phase 8: Testing** (`08_tests.md`)
- Follow prompts to generate test files

### Critical Manual Steps

Between AI-generated code, you must perform these actions:

1. **After Phase 1**: Start database with `docker compose up -d`
2. **After Prompt_07**: Create tables with `python app/database/create_tables.py`
3. **After Phase 7**: Initialize packages with `python init_packages.py`
4. **Environment Setup**: Configure `app/.env` with API keys before running

### Using the Prompts

Simply copy-paste each prompt into your AI assistant (e.g., Cursor, GitHub Copilot, Claude) in the order specified. The prompts are self-contained and include all necessary context.

## ğŸ“ License

This project is provided as-is for educational and personal use.

## ğŸ™ Acknowledgments

- Built as part of the AI Dev Tools Zoomcamp
- Inspired by modern AI engineering practices and vibe coding workflows

---

**Happy coding! ğŸš€** If you have questions or improvements, feel free to open an issue or contribute to the project.
